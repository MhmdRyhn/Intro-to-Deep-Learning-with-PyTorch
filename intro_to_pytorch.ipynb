{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(2000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 2 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "weights:\n",
      " tensor([[0.2868, 0.2063, 0.4451, 0.3593, 0.7204]])\n",
      "bias:\n",
      " tensor([[0.0731]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "# features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "print('features:\\n', features)\n",
    "# Same as features' dimension\n",
    "weights = torch.rand_like(features)\n",
    "print('weights:\\n', weights)\n",
    "# Bias term\n",
    "bias = torch.rand((1,1))\n",
    "print('bias:\\n', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_xi = torch.sum(features * weights)\n",
    "# This can also be used\n",
    "# wi_xi = (features * weights).sum()\n",
    "y_hat = sigmoid_activation(wi_xi + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8072]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For multiple layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features2 = sigmoid_activation(torch.mm(features, W1) + B1)\n",
    "y_hat = sigmoid_activation(torch.mm(features2, W2) + B2)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=False, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56d44a1e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "hidden_units = 256\n",
    "size, input_units = inputs.shape\n",
    "output_units = 10\n",
    "\n",
    "w1 = torch.randn(input_units, hidden_units)\n",
    "# b1 = torch.randn(1, hidden_units)\n",
    "b1 = torch.randn(hidden_units)\n",
    "\n",
    "w2 = torch.randn(hidden_units, output_units)\n",
    "# b2 = torch.randn(1, output_units)\n",
    "b2 = torch.randn(output_units)\n",
    "\n",
    "h = sigmoid_activation(torch.mm(inputs, w1) + b1)\n",
    "output = sigmoid_activation(torch.mm(h, w2) + b2)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "probabilities = softmax(output)\n",
    "print(probabilities.shape)\n",
    "# Check for each image that if the summation of all the 10 probabilities is equal to 1\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch (OOP style):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(in_features=784, out_features=256)\n",
    "        self.output = nn.Linear(in_features=256, out_features=10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of building neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(in_features=784, out_features=256)\n",
    "        self.output = nn.Linear(in_features=256, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "So far we've only been looking at the softmax activation, but in general any function can be used as an activation function. The only requirement is that for a network to approximate a non-linear function, **the activation functions must be non-linear**. A few more examples of common activation functions: **Tanh (hyperbolic tangent), and ReLU (rectified linear unit)**. In practice, the **ReLU function is used almost exclusively as the activation function for hidden layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc_output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc_output = nn.Linear(in_features=64, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc_output(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural network using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import helper\n",
    "\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[1], output_size), \n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "images.shape\n",
    "# images[0,:]\n",
    "probabilities = model.forward(images[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential and collections.OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "        ('softmax', nn.Softmax(dim=1))\n",
    "    ])\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=64, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2941, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model.forward(images)\n",
    "loss = criterion(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss.\n",
    "\n",
    "**Observation:** The following cell is equivalent to the above cell. Because, **nn.CrossEntropyLoss** is equal to **nn.LogSoftmax** followed by **nn.NLLLoss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2998, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10), \n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "log_probabilities = model.forward(images)\n",
    "loss = criterion(log_probabilities, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3748,  0.0901],\n",
      "        [-0.1801, -0.3838]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "# x = torch.randn(2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8902, 0.0081],\n",
      "        [0.0324, 0.1473]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5195, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6874,  0.0451],\n",
      "        [-0.0900, -0.1919]])\n",
      "tensor([[ 0.6874,  0.0451],\n",
      "        [-0.0900, -0.1919]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 -> Training Loss: 1.018921000616891\n",
      "Epoch -  2 -> Training Loss: 0.37797287924648093\n",
      "Epoch -  3 -> Training Loss: 0.32043989144090906\n",
      "Epoch -  4 -> Training Loss: 0.29104651386009606\n",
      "Epoch -  5 -> Training Loss: 0.2671191857408867\n",
      "Epoch -  6 -> Training Loss: 0.24618872241583714\n",
      "Epoch -  7 -> Training Loss: 0.22693421984754647\n",
      "Epoch -  8 -> Training Loss: 0.20878734125860973\n",
      "Epoch -  9 -> Training Loss: 0.19241259354692913\n",
      "Epoch -  10 -> Training Loss: 0.1777948343129491\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# images, labels = next(iter(trainloader))\n",
    "# images = images.view(images.shape[0], -1)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    training_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clearing previous gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        # Call .backward() to calculate backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "    else:\n",
    "        print('Epoch - ', i+1, '-> Training Loss:', training_loss/len(trainloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2770e-05, 1.5201e-03, 9.7647e-01, 1.3797e-02, 1.8437e-05, 5.2858e-06,\n",
       "         2.9625e-04, 7.7380e-03, 1.2649e-04, 7.0479e-06]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "image = images[0].view(1, 784)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(image)\n",
    "ps = torch.exp(logps)\n",
    "ps\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56bc17b550>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.view(28,28).numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/rayhan/pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "trainset = datasets.FashionMNIST(\n",
    "    '~/pytorch/F_MNIST_data/', \n",
    "    download=True, train=True, \n",
    "    transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.FashionMNIST(\n",
    "    '~/pytorch/F_MNIST_data/', \n",
    "    download=True, train=False, \n",
    "    transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -> Training Loss: 495.7854176610708\n",
      "Epoch 2 -> Training Loss: 386.90747368335724\n",
      "Epoch 3 -> Training Loss: 352.41321751475334\n",
      "Epoch 4 -> Training Loss: 337.2679408863187\n",
      "Epoch 5 -> Training Loss: 322.57275868952274\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clearing the gradient of the previous step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "    else:\n",
    "        print('Epoch', epoch+1, '-> Training Loss:', training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
