{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(2000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 2 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "weights:\n",
      " tensor([[0.2868, 0.2063, 0.4451, 0.3593, 0.7204]])\n",
      "bias:\n",
      " tensor([[0.0731]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "# features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "print('features:\\n', features)\n",
    "# Same as features' dimension\n",
    "weights = torch.rand_like(features)\n",
    "print('weights:\\n', weights)\n",
    "# Bias term\n",
    "bias = torch.rand((1,1))\n",
    "print('bias:\\n', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_xi = torch.sum(features * weights)\n",
    "# This can also be used\n",
    "# wi_xi = (features * weights).sum()\n",
    "y_hat = sigmoid_activation(wi_xi + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8072]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For multiple layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features2 = sigmoid_activation(torch.mm(features, W1) + B1)\n",
    "y_hat = sigmoid_activation(torch.mm(features2, W2) + B2)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=False, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1980577cec8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdbUlEQVR4nO3de7BlZXkn4N8rKG2ocK0gSRy5JUgViYIQQckgtKWjFeMlwoQ/TKioqZjRIRiZMpWogzFTpZWJVxxJxYokGoekoEKSCfFSCgIiSdloGIwohm6RioLIcBFoFfzmj7066bTndPfZa3fvs7/9PFW7Vu+11ru/txeL/p21z7pUay0AQD8eM+8GAIDZEu4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0Jl9593AnlBVm5MckGTLnFsBgGkdmeT+1tpRay3sMtwzCfZDhhcALJVev5bfMu8GAGAGtkxTNNdwr6onVtUfV9W/VNV3qmpLVb2zqg6eZ18AsMjm9rV8VR2T5PokhyX5qyS3JHl6kt9I8ryqOq219q159QcAi2qeR+7/K5NgP6+19uLW2m+11jYmeUeSJyf5H3PsDQAWVrXW9v6gVUcn+edMfpdwTGvt+9st++EkX09SSQ5rrT04xedvSvK02XQLAHNzY2vtpLUWzetr+Y3D9GPbB3uStNYeqKpPJ3luklOTfGK1DxlCfCXHzaRLAFhA8/pa/snD9MurLL91mB67F3oBgK7M68j9wGF63yrLt80/aGcfstpXFb6WB2CZrdfr3GuY7v0TAgBgwc0r3LcdmR+4yvIDdlgPANhN8wr3Lw3T1X6n/pPDdLXfyQMAq5hXuF81TJ9bVf+uh+FSuNOSPJzkhr3dGAAsurmEe2vtn5N8LJMn3rx6h8VvTrJ/kj+d5hp3AFh283wq3H/J5Paz766qZyf5YpJTkpyZydfxvzPH3gBgYc3tbPnh6P3kJJdkEuqvS3JMkncneYb7ygPAdOb6PPfW2teS/Mo8ewCA3qzX69wBgCkJdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM7sO+8GYBb233//UfXXXnvt1LUnnnjiqLFvu+22qWuPOeaYUWMDfXLkDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCd8Tx3uvD6179+VP1Tn/rUqWtba6PGPvzww6eu/fSnPz1q7GX17ne/e1T9Zz7zmalrb7/99lFjw+6Y25F7VW2pqrbK6xvz6gsAFt28j9zvS/LOFeZ/e283AgC9mHe439tau3DOPQBAV5xQBwCdmfeR+35V9bIkT0ryYJKbklzTWnt0vm0BwOKad7gfnuSDO8zbXFW/0lr71K6Kq2rTKouOG90ZACyoeX4t/4Ekz84k4PdP8tNJ/jDJkUn+rqqmvzYJAJbY3I7cW2tv3mHWzUleVVXfTvK6JBcmeckuPuOkleYPR/RPm0GbALBw1uMJdRcP09Pn2gUALKj1GO53DdP959oFACyo9Rjuzximt821CwBYUHMJ96o6vqoOWWH+EUkuGt5+aO92BQB9mNcJdWcn+a2quirJ5iQPJDkmyc8l2ZDkyiT/c069AcBCm1e4X5XkyUlOzORr+P2T3Jvkukyue/9gG/uoLQBYUtVjhroUbjGdcsopU9deffXVo8Z+3OMeN3VtVY0ae1H/H1zWv3eS3HnnnVPXnnfeeaPGvuyyy0bVs3BuXO2y751ZjyfUAQAjCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DO7DvvBmCbgw8+eOraMc9jn7fPfe5zU9fecsstM+xkbcY+z/2JT3ziqPqf/dmfHVU/xuGHHz517Vvf+tZRY3ueO7vDkTsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnPPKVdeM5z3nO1LVjHz86xsaNG0fVX3311bNphN02dpuffvrpU9ceccQRcxv7mmuuGTU2i8OROwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0xvPcWTdOPfXUqWtbazPsZG08j33xHHbYYXMb+6tf/eqoes9kZ3c4cgeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMR77CSK95zWtG1V900UUz6mS5HHXUUVPXHnHEETPsZG02b948t7FZHo7cAaAzMwn3qjqrqt5TVddW1f1V1arqQ7uoeWZVXVlV91TVQ1V1U1WdX1X7zKInAFhWs/pa/g1Jnprk20nuSHLczlauqhcluTzJ1iR/nuSeJD+f5B1JTkty9oz6AoClM6uv5V+b5NgkByT59Z2tWFUHJPmjJI8mOaO19orW2n9LckKSzyQ5q6rOmVFfALB0ZhLurbWrWmu3ttbabqx+VpIfSXJpa+2z233G1ky+AUh28QMCALC6eZxQt3GYfmSFZdckeSjJM6tqv73XEgD0Yx6Xwj15mH55xwWttUeqanOS45McneSLO/ugqtq0yqKd/s4fAHo2jyP3A4fpfass3zb/oL3QCwB0Zz3exKaG6S5/f99aO2nFD5gc0T9tlk0BwKKYx5H7tiPzA1dZfsAO6wEAazCPcP/SMD12xwVVtW+So5I8kuS2vdkUAPRiHuH+yWH6vBWWnZ7kh5Jc31r7zt5rCQD6MY9wvyzJ3UnOqaqTt82sqg1Jfm94+7459AUAXZjJCXVV9eIkLx7eHj5Mn1FVlwx/vru1dkGStNbur6pfzSTkr66qSzO5/ewLM7lM7rJMbkkLAExhVmfLn5Dk3B3mHT28kuSrSS7YtqC1dkVVPSvJ7yR5aZINSb6S5DeTvHs373QHAKygesxRl8Itpl/8xV+cuvbDH/7wDDtZm02bVruX0u75kz/5k6lr3/ve944ae4yxz6F/2cteNqr+sY997NS1GzZsGDX29773valrN27cuOuVduL6668fVc/CuXG1y753xvPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOjOr57nDaHfeeefUtVu3bh019uMf//ipa08++eRRY4+pf8973jNqbKbzgQ98YOrazZs3z7ATWJkjdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDoTLXW5t3DzFXVpiRPm3cf7D1HHnnkqPovfOELU9eOeRZ8kizq/4NVNap+Uf/eybi/+8MPPzxq7D/7sz+buvb3f//3R4196623jqpnKje21k5aa5EjdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM545CtdOOqoo0bV33zzzVPXLvIjXz/3uc9NXbt169ZRYx988MGj6o877rhR9WOMeeTrPP97P/jgg6PqL7/88qlr3/zmN48ae8uWLaPqF5hHvgIAwh0AuiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAznufOuvGkJz1p6tobbrhh1NhPeMITpq4d82zvZNzzvS+++OJRY5933nlT1z766KOjxt6wYcOo+sMPP3xU/RgHHXTQ1LUXXHDBqLHHPMf+xBNPHDX2mH395ptvHjX2U57ylFH1C8zz3AGAGYV7VZ1VVe+pqmur6v6qalX1oVXWPXJYvtrr0ln0BADLat8Zfc4bkjw1ybeT3JFkd743+sckV6wwf9x3NwCw5GYV7q/NJNS/kuRZSa7ajZrPt9YunNH4AMBgJuHeWvvXMB97chEAMM6sjtyn8WNV9WtJDk3yrSSfaa3dtJYPGM6KX8n0p5MCwIKbZ7g/Z3j9q6q6Osm5rbXb59IRAHRgHuH+UJK3ZHIy3W3DvKckuTDJmUk+UVUntNYe3NUHrXbtn+vcAVhme/0699baXa21N7XWbmyt3Tu8rkny3CR/n+Qnkrxyb/cFAL1YNzexaa09kuT9w9vT59kLACyydRPug28O0/3n2gUALLD1Fu6nDtPbdroWALCqvR7uVXVKVT1uhfkbM7kZTpKseOtaAGDXZnK2fFW9OMmLh7fbHtX0jKq6ZPjz3a21bY9CeluS44fL3u4Y5j0lycbhz29srV0/i74AYBnN6lK4E5Kcu8O8o4dXknw1ybZw/2CSlyT5mSTPT/LYJHcm+YskF7XWrp1RTwCwlDzPnXXjnHPOmbr2wx/+8Aw7WZu3ve1to+rvuOOOXa+0ive+972jxma5/PVf//Wo+he84AUz6mTt3v72t09de8EFF+x6pfXL89wBAOEOAN0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ3xyFdmZsOGDaPqN23aNHXtcccdN2rsMfbZZ5+5jQ170/333z917f777z9q7FtuuWXq2pNOWvMTU/+drVu3jqofySNfAQDhDgDdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0Jl9590A/fjRH/3RUfXzfCb7ddddN7exYVHceuutU9eecMIJo8ZurU1d+/3vf3/U2IvIkTsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnPPIVkjz96U+fdwuwx51xxhmj6k888cSpa8c8sjVJ7r333qlrv/vd744aexE5cgeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAznieOzOzefPmUfVvectbpq5905veNGrs/fbbb+rarVu3jhr7Xe9619S1f/AHfzBq7LvuumtUPYvlrLPOmtvYVTWq/mMf+9iMOlkOo4/cq+rQqnplVf1lVX2lqh6uqvuq6rqqekVVrThGVT2zqq6sqnuq6qGquqmqzq+qfcb2BADLbBZH7mcneV+Srye5KsntSZ6Q5BeSvD/J86vq7NZa21ZQVS9KcnmSrUn+PMk9SX4+yTuSnDZ8JgAwhVmE+5eTvDDJ37bWvr9tZlX9dpJ/SPLSTIL+8mH+AUn+KMmjSc5orX12mP/GJJ9MclZVndNau3QGvQHA0hn9tXxr7ZOttb/ZPtiH+d9IcvHw9oztFp2V5EeSXLot2If1tyZ5w/D218f2BQDLak+fLf+9YfrIdvM2DtOPrLD+NUkeSvLMqpr+DCcAWGJ77Gz5qto3yS8Pb7cP8icP0y/vWNNae6SqNic5PsnRSb64izE2rbLouLV1CwD92JNH7m9N8lNJrmytfXS7+QcO0/tWqds2/6A91RgA9GyPHLlX1XlJXpfkliS/tNbyYdp2ulaS1tpJq4y/KcnT1jguAHRh5kfuVfXqJO9K8k9Jzmyt3bPDKtuOzA/Myg7YYT0AYA1mGu5VdX6Si5LcnEmwf2OF1b40TI9doX7fJEdlcgLebbPsDQCWxczCvapen8lNaD6fSbCvdl/LTw7T562w7PQkP5Tk+tbad2bVGwAsk5mE+3ADmrcm2ZTk2a21u3ey+mVJ7k5yTlWdvN1nbEjye8Pb982iLwBYRqNPqKuqc5P8biZ3nLs2yXkrPCBgS2vtkiRprd1fVb+aSchfXVWXZnL72RdmcpncZZnckhYAmMIszpY/apjuk+T8Vdb5VJJLtr1prV1RVc9K8juZ3J52Q5KvJPnNJO/e/j70AMDaVI856lK4xXTYYYdNXfupT31q1NjHHvsD53futrGPshzz/+ADDzwwauwvfOELU9d+/OMfHzX2DTfcMKr+5S9/+dS1P/7jPz5q7EV18skn73qlnXjMY6b/Te5nP/vZXa+0Ey960Yumrl3wRxvfuNpl3zuzp28/CwDsZcIdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM57nThfGPAs+SV71qldNXfua17xm1NiHHnroqPp5medz7OdtzN99kf/eDzzwwNS1Bx100Aw7WSqe5w4ACHcA6I5wB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxHvsJIhxxyyKj6E044Yeral7/85aPGfulLXzp17X777Tdq7EX+t2eej3y97rrrpq697LLLRo19xRVXTF37ta99bdTYS8wjXwEA4Q4A3RHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZz3MHgPXL89wBAOEOAN0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0ZHe5VdWhVvbKq/rKqvlJVD1fVfVV1XVW9oqoes8P6R1ZV28nr0rE9AcAy23cGn3F2kvcl+XqSq5LcnuQJSX4hyfuTPL+qzm6ttR3q/jHJFSt83s0z6AkAltYswv3LSV6Y5G9ba9/fNrOqfjvJPyR5aSZBf/kOdZ9vrV04g/EBgO2M/lq+tfbJ1trfbB/sw/xvJLl4eHvG2HEAgN0ziyP3nfneMH1khWU/VlW/luTQJN9K8pnW2k17uB8A6N4eC/eq2jfJLw9vP7LCKs8ZXtvXXJ3k3Nba7bs5xqZVFh23m20CQHf25KVwb03yU0mubK19dLv5DyV5S5KTkhw8vJ6Vycl4ZyT5RFXtvwf7AoCu1Q+exD6DD606L8m7ktyS5LTW2j27UbNvkuuSnJLk/Nbau0aMvynJ06atB4B14sbW2klrLZr5kXtVvTqTYP+nJGfuTrAnSWvtkUwunUuS02fdFwAsi5mGe1Wdn+SiTK5VP3M4Y34tvjlMfS0PAFOaWbhX1euTvCPJ5zMJ9rum+JhTh+lts+oLAJbNTMK9qt6YyQl0m5I8u7V2907WPaWqHrfC/I1JXju8/dAs+gKAZTT6UriqOjfJ7yZ5NMm1Sc6rqh1X29Jau2T489uSHD9c9nbHMO8pSTYOf35ja+36sX0BwLKaxXXuRw3TfZKcv8o6n0pyyfDnDyZ5SZKfSfL8JI9NcmeSv0hyUWvt2hn0BABLa49cCjdvLoUDoBPr41I4AGC+hDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0Bneg33I+fdAADMwJHTFO074ybWi/uH6ZZVlh83TG/Z8610wzabju02Hdtt7Wyz6azn7XZk/i3P1qRaa7NtZQFU1aYkaa2dNO9eFoVtNh3bbTq229rZZtPpdbv1+rU8ACwt4Q4AnRHuANAZ4Q4AnRHuANCZpTxbHgB65sgdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADqzVOFeVU+sqj+uqn+pqu9U1ZaqemdVHTzv3tarYRu1VV7fmHd/81JVZ1XVe6rq2qq6f9geH9pFzTOr6sqquqeqHqqqm6rq/KraZ2/1PW9r2W5VdeRO9r1WVZfu7f7noaoOrapXVtVfVtVXqurhqrqvqq6rqldU1Yr/ji/7/rbW7dbb/tbr89x/QFUdk+T6JIcl+atMnt379CS/keR5VXVaa+1bc2xxPbsvyTtXmP/tvd3IOvKGJE/NZBvckX97JvSKqupFSS5PsjXJnye5J8nPJ3lHktOSnL0nm11H1rTdBv+Y5IoV5t88w77Ws7OTvC/J15NcleT2JE9I8gtJ3p/k+VV1dtvujmT2tyRTbLdBH/tba20pXkk+mqQl+a87zH/7MP/iefe4Hl9JtiTZMu8+1tsryZlJfjJJJTlj2Ic+tMq6ByS5K8l3kpy83fwNmfzA2ZKcM++/0zrcbkcOyy+Zd99z3mYbMwnmx+ww//BMAqsleel28+1v0223rva3pfhavqqOTvLcTILqvTss/u9JHkzyS1W1/15ujQXVWruqtXZrG/5V2IWzkvxIkktba5/d7jO2ZnIkmyS/vgfaXHfWuN1I0lr7ZGvtb1pr399h/jeSXDy8PWO7Rfa3TLXdurIsX8tvHKYfW+E/9ANV9elMwv/UJJ/Y280tgP2q6mVJnpTJD0I3JbmmtfbofNtaGNv2v4+ssOyaJA8leWZV7dda+87ea2th/FhV/VqSQ5N8K8lnWms3zbmn9eJ7w/SR7ebZ33Ztpe22TRf727KE+5OH6ZdXWX5rJuF+bIT7Sg5P8sEd5m2uql9prX1qHg0tmFX3v9baI1W1OcnxSY5O8sW92diCeM7w+ldVdXWSc1trt8+lo3WgqvZN8svD2+2D3P62EzvZbtt0sb8txdfySQ4cpvetsnzb/IP2Qi+L5gNJnp1JwO+f5KeT/GEmv5/6u6p66vxaWxj2v+k8lOQtSU5KcvDwelYmJ0edkeQTS/6rtLcm+akkV7bWPrrdfPvbzq223bra35Yl3HelhqnfA+6gtfbm4XdXd7bWHmqt3dxae1UmJyI+PsmF8+2wC/a/FbTW7mqtvam1dmNr7d7hdU0m37L9fZKfSPLK+XY5H1V1XpLXZXLVzy+ttXyYLt3+trPt1tv+tizhvu0n1QNXWX7ADuuxa9tOSDl9rl0sBvvfDLXWHsnkUqZkCfe/qnp1kncl+ackZ7bW7tlhFfvbCnZju61oUfe3ZQn3Lw3TY1dZ/pPDdLXfyfOD7hqmC/M11Rytuv8Nv/87KpMTe27bm00tuG8O06Xa/6rq/CQXZXLN9ZnDmd87sr/tYDe3284s3P62LOF+1TB97gp3JfrhTG7q8HCSG/Z2YwvsGcN0af6BGOGTw/R5Kyw7PckPJbl+ic9cnsapw3Rp9r+qen0mN6H5fCYBddcqq9rftrOG7bYzC7e/LUW4t9b+OcnHMjkJ7NU7LH5zJj+N/Wlr7cG93Nq6VlXHV9UhK8w/IpOfgpNkp7dcJUlyWZK7k5xTVSdvm1lVG5L83vD2ffNobD2rqlOq6nErzN+Y5LXD26XY/6rqjZmcCLYpybNba3fvZHX722At2623/a2W5V4SK9x+9otJTsnkjllfTvLM5vaz/05VXZjktzL55mNzkgeSHJPk5zK529WVSV7SWvvuvHqcl6p6cZIXD28PT/KfMvmp/tph3t2ttQt2WP+yTG4HemkmtwN9YSaXLV2W5D8vw41d1rLdhsuPjk9ydSa3qk2Sp+TfruN+Y2ttW1h1q6rOTXJJkkeTvCcr/658S2vtku1qln5/W+t2625/m/ct8vbmK8l/yOTSrq8n+W6Sr2ZygsUh8+5tPb4yuQzkf2dyZum9mdz44ZtJPp7JdaI17x7nuG0uzORs49VeW1aoOS2TH4j+Xya/Bvq/mRwR7DPvv8963G5JXpHk/2RyZ8lvZ3I71dszuVf6f5z332UdbbOW5Gr727jt1tv+tjRH7gCwLJbid+4AsEyEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGf+P/hoJD0qDHaOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "hidden_units = 256\n",
    "size, input_units = inputs.shape\n",
    "output_units = 10\n",
    "\n",
    "w1 = torch.randn(input_units, hidden_units)\n",
    "# b1 = torch.randn(1, hidden_units)\n",
    "b1 = torch.randn(hidden_units)\n",
    "\n",
    "w2 = torch.randn(hidden_units, output_units)\n",
    "# b2 = torch.randn(1, output_units)\n",
    "b2 = torch.randn(output_units)\n",
    "\n",
    "h = sigmoid_activation(torch.mm(inputs, w1) + b1)\n",
    "output = sigmoid_activation(torch.mm(h, w2) + b2)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "probabilities = softmax(output)\n",
    "print(probabilities.shape)\n",
    "# Check for each image that if the summation of all the 10 probabilities is equal to 1\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch (OOP style):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(in_features=784, out_features=256)\n",
    "        self.output = nn.Linear(in_features=256, out_features=10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of building neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(in_features=784, out_features=256)\n",
    "        self.output = nn.Linear(in_features=256, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "So far we've only been looking at the softmax activation, but in general any function can be used as an activation function. The only requirement is that for a network to approximate a non-linear function, **the activation functions must be non-linear**. A few more examples of common activation functions: **Tanh (hyperbolic tangent), and ReLU (rectified linear unit)**. In practice, the **ReLU function is used almost exclusively as the activation function for hidden layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc_output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc_output = nn.Linear(in_features=64, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc_output(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural network using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import helper\n",
    "\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[1], output_size), \n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "images.shape\n",
    "# images[0,:]\n",
    "probabilities = model.forward(images[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential and collections.OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "        ('softmax', nn.Softmax(dim=1))\n",
    "    ])\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=64, bias=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2941, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model.forward(images)\n",
    "loss = criterion(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss.\n",
    "\n",
    "**Observation:** The following cell is equivalent to the above cell. Because, **nn.CrossEntropyLoss** is equal to **nn.LogSoftmax** followed by **nn.NLLLoss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2998, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10), \n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "log_probabilities = model.forward(images)\n",
    "loss = criterion(log_probabilities, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6356,  0.0943],\n",
      "        [-1.8824, -0.1884]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "# x = torch.randn(2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4040, 0.0089],\n",
      "        [3.5433, 0.0355]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9979, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3178,  0.0471],\n",
      "        [-0.9412, -0.0942]])\n",
      "tensor([[-0.3178,  0.0471],\n",
      "        [-0.9412, -0.0942]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.002637695163679\n",
      "Training Loss: 0.3765400700541193\n",
      "Training Loss: 0.31901792679101165\n",
      "Training Loss: 0.287627435275423\n",
      "Training Loss: 0.26293736365018117\n",
      "Training Loss: 0.2408696306365004\n",
      "Training Loss: 0.22221980906371622\n",
      "Training Loss: 0.2050559350819603\n",
      "Training Loss: 0.19047781141169035\n",
      "Training Loss: 0.17715509816495847\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# images, labels = next(iter(trainloader))\n",
    "# images = images.view(images.shape[0], -1)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    training_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clearing previous gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        # Call .backward() to calculate backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "    else:\n",
    "        print('Training Loss:', training_loss/len(trainloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2653e-02, 3.4436e-05, 8.1184e-01, 9.5075e-02, 1.4501e-04, 1.3519e-03,\n",
       "         1.4607e-03, 3.1473e-05, 7.6433e-02, 9.7586e-04]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "image = images[0].view(1, 784)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(image)\n",
    "ps = torch.exp(logps)\n",
    "ps\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x198057e4fc8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdbklEQVR4nO3df9BtdV0v8PcnDsiPCcQf5VgmYAozlD+Akh9zj4DllWsZIlyt0ajRprx0BdNrWaJY3dTQiwr3SoMWg05So5NORuoNjoKphRBwmVQ0IbUkBASBww+B7/1j71On0/Occ5699nnW83z36zWzZ5291vrs7+csFuf9rP2sH9VaCwDQj+8ZuwEAYL6EOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0ZsPYDewKVXVjkn2T3DRyKwAwqwOSfKe1duBKC7sM90yC/VHTFwAslF6/lr9p7AYAYA5umqVo1HCvqh+sqj+sqn+uqvur6qaqekdV7T9mXwCwno32tXxVPSnJZ5J8X5KPJPlikh9PcnqS51bVMa2128bqDwDWqzGP3P9PJsH+ytbaia2132itHZ/knCQHJ/mfI/YGAOtWtdZWf9Cqg5L8Qya/S3hSa+3hrZZ9b5JvJqkk39dau2eGz78qyWHz6RYARnN1a+3wlRaN9bX88dPpJ7YO9iRprd1VVX+d5DlJjkxy6XIfMg3xpRwyly4BYB0a62v5g6fTG5ZZ/uXp9Cmr0AsAdGWsI/f9ptM7l1m+Zf4jt/chy31V4Wt5ABbZWr3OvabT1T8hAADWubHCfcuR+X7LLN93m/UAgJ00Vrh/aTpd7nfqT55Ol/udPACwjLHCfdN0+pyq+nc9TC+FOybJvUk+t9qNAcB6N0q4t9b+IcknMnnizWnbLH5Tkn2SXDTLNe4AsOjGfCrcf8vk9rPvqqpnJ/lCkmcmOS6Tr+N/a8TeAGDdGu1s+enR+xFJLswk1F+d5ElJ3pXkKPeVB4DZjPo899ba15P84pg9AEBv1up17gDAjIQ7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRmw9gNAIvp9NNPH1R/9tlnz1x78MEHDxr7xhtvHFQPu5ojdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM545CswkxNPPHFQ/e///u8Pqt99991nrv2Jn/iJQWNfe+21M9deeeWVg8ZurQ2qZzE4cgeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzlSPzwauqquSHDZ2H9CzTZs2Dao/9thj59PIOnP55ZcPqn/e8543c+3dd989aGxGcXVr7fCVFo125F5VN1VVW+Z181h9AcB6t2Hk8e9M8o4l5vvxEgBmNHa439FaO2vkHgCgK06oA4DOjH3k/oiqekmSH0pyT5LrklzeWnto3LYAYP0aO9wfl+R928y7sap+sbX2qR0VT8+KX8ohgzsDgHVqzK/l/yjJszMJ+H2S/GiSP0hyQJK/rKqnjdcaAKxfox25t9betM2s65P8SlXdneTVSc5K8oIdfMaS1/65zh2ARbYWT6g7fzrdOGoXALBOrcVwv2U63WfULgBgnVqL4X7UdPrVUbsAgHVqlHCvqkOr6lFLzH9ikvOmb9+/ul0BQB/GOqHulCS/UVWbktyY5K4kT0ryvCR7JrkkydtG6g0A1rWxwn1TkoOTPCOTr+H3SXJHkk9nct37+1qPj6sDgFUwSrhPb1Czw5vUALvWSSedNHPt0UcfPcdOVtf9998/qH6PPfaYuXbjxmEXAp122mkz1771rW8dNDbrx1o8oQ4AGEC4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdGaU57kD8/H4xz9+UP3rX//6mWuHPNM8Se66665B9WefffbMtW9729sGjf1Xf/VXM9ceffTRg8bec889B9WzGBy5A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdMYjX2FkQx7b+tGPfnTQ2M94xjMG1Q/x2te+dlD9+eefP6dOVu7mm28ebewXvvCFM9e+6U1vmmMnrGWO3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM57nDgNt2DDsf6NLL7105tpDDjlk0NhDXHfddYPqL7roojl1slgOPPDAmWv32muvQWPfe++9g+pZPY7cAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOuORrzDQ6aefPqh+zMe23nnnnTPXnnrqqYPG3rx586D6RXXffffNXPvQQw/NsRPWMkfuANCZuYR7VZ1cVedW1RVV9Z2qalX1/h3UHF1Vl1TV7VW1uaquq6ozqmq3efQEAItqXl/Lvz7J05LcneQbSbb7PWNV/UySDyW5L8mfJLk9yU8nOSfJMUlOmVNfALBw5vW1/KuSPCXJvklesb0Vq2rfJBckeSjJsa21l7XW/keSpyf5bJKTq+rFc+oLABbOXMK9tbaptfbl1lrbidVPTvLYJBe31j6/1Wfcl8k3AMkOfkAAAJY3xgl1x0+nH1ti2eVJNic5uqoesXotAUA/xrgU7uDp9IZtF7TWHqyqG5McmuSgJF/Y3gdV1VXLLBrv2iIAGNkYR+77TafLXWC7Zf4jV6EXAOjOWryJTU2nO/z9fWvt8CU/YHJEf9g8mwKA9WKMI/ctR+b7LbN8323WAwBWYIxw/9J0+pRtF1TVhiQHJnkwyVdXsykA6MUY4X7ZdPrcJZZtTLJ3ks+01u5fvZYAoB9jhPsHk9ya5MVVdcSWmVW1Z5Lfnb599wh9AUAX5nJCXVWdmOTE6dvHTadHVdWF0z/f2lp7TZK01r5TVb+USch/sqouzuT2s8/P5DK5D2ZyS1oAYAbzOlv+6Um2ff7jQdNXkvxjktdsWdBa+3BVPSvJbyV5YZI9k3wlya8leddO3ukOAFjCXMK9tXZWkrNWWPPXSf7LPMaHMT3mMY8Zbewhz2NPkmOPPXbm2muuuWbQ2MzmgQcemLnWcdPi8Dx3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzszree6wsC644IJB9bvvvvvMteecc86gsf/pn/5pUP2i2n///Ucbe8gjhvfYY49BY3/3u98dVM/qceQOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ2p1trYPcxdVV2V5LCx+4BdbePGjYPqX/KSl8ypk5X7u7/7u0H1733ve2euHfpc81tuuWXm2r322mvQ2B/4wAdmrv25n/u5QWMziqtba4evtMiROwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGc2jN0ArHc/9VM/Naj+vPPOm7n2B37gBwaNvWHD+v0n4HWve93MtZdeeumgsYc+tnWIa6+9drSxWT8cuQNAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ9bvw5xhjl7xilfMXPv2t7990NhDng1+1113DRr7yiuvHFQ/xMaNGwfVP+EJT5i59hd+4RcGjT2m2267bewWWAccuQNAZ+YS7lV1clWdW1VXVNV3qqpV1fuXWfeA6fLlXhfPoycAWFTz+lr+9UmeluTuJN9IcshO1Fyb5MNLzL9+Tj0BwEKaV7i/KpNQ/0qSZyXZtBM117TWzprT+ADA1FzCvbX2r2FeVfP4SABgRmOeLf/4qvrlJI9OcluSz7bWrlvJB1TVVcss2plfCwBAl8YM95+cvv5VVX0yyamtta+N0hEAdGCMcN+c5HcyOZnuq9N5T01yVpLjklxaVU9vrd2zow9qrR2+1PzpEf1hc+kWANaZVb/OvbV2S2vtDa21q1trd0xflyd5TpK/SfLDSV6+2n0BQC/WzE1sWmsPJnnP9O2wW1cBwAJbM+E+9a3pdJ9RuwCAdWythfuR0+lXt7sWALCsVQ/3qnpmVe2xxPzjM7kZTpIseetaAGDH5nK2fFWdmOTE6dvHTadHVdWF0z/f2lp7zfTPb01y6PSyt29M5z01yfHTP5/ZWvvMPPoCgEU0r0vhnp7k1G3mHTR9Jck/JtkS7u9L8oIkP5bkhCS7J/mXJH+a5LzW2hVz6gkAFlK11sbuYe5c5754fu/3fm9Q/Wtf+9qZazdv3jxo7DPPPHPm2osuumjQ2N/+9rcH1Q9x7rnnDqr/1V/91Tl1sro+8pGPDKo/6aSTZq59+OGHB43NKK5e7p4u27PWTqgDAAYS7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQmXk9zx0GG/IoyyGPbE2SO++8c+bao446atDYN9xww6D69errX//62C2M4u1vf/ugeo9tZWc4cgeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAznieO3Nz0EEHDaq/4IILZq7dbbfdBo29adOmmWsX9XnsGzduHFT/xje+cU6drC8/+7M/O6j+iiuumFMn9MyROwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeqtTZ2D3NXVVclOWzsPhbNa17zmkH1Z5999sy1119//aCxjzzyyJlr77nnnkFjj+nNb37zzLVD/3tv2DDsidP33XffzLX333//oLH322+/mWsffvjhQWM/5jGPmbn229/+9qCxGcXVrbXDV1rkyB0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOjPsgcqwRtx4442D6oc8k33vvfceNPYb3vCGmWtf9KIXDRr7iU984sy1VTVo7KH/zV760pfOXPv1r3990Nif//znZ6597GMfO2jsE044YebaP/7jPx40NuvH4CP3qnp0Vb28qv6sqr5SVfdW1Z1V9emqellVLTlGVR1dVZdU1e1VtbmqrquqM6pqt6E9AcAim8eR+ylJ3p3km0k2Jflaku9PclKS9yQ5oapOaa21LQVV9TNJPpTkviR/kuT2JD+d5Jwkx0w/EwCYwTzC/YYkz0/yF621h7fMrKrfTPK3SV6YSdB/aDp/3yQXJHkoybGttc9P55+Z5LIkJ1fVi1trF8+hNwBYOIO/lm+tXdZa+/Otg306/+Yk50/fHrvVopOTPDbJxVuCfbr+fUleP337iqF9AcCi2tVny393On1wq3nHT6cfW2L9y5NsTnJ0VT1iVzYGAL3aZWfLV9WGJD8/fbt1kB88nd6wbU1r7cGqujHJoUkOSvKFHYxx1TKLDllZtwDQj1155P6WJD+S5JLW2se3mr/fdHrnMnVb5j9yVzUGAD3bJUfuVfXKJK9O8sUkK70YdcvFs227ayVprR2+zPhXJTlsheMCQBfmfuReVacleWeSv09yXGvt9m1W2XJkvl+Wtu826wEAKzDXcK+qM5Kcl+T6TIL95iVW+9J0+pQl6jckOTCTE/C+Os/eAGBRzC3cq+rXM7kJzTWZBPsty6x62XT63CWWbUyyd5LPtNbun1dvALBI5hLu0xvQvCXJVUme3Vq7dTurfzDJrUleXFVHbPUZeyb53enbd8+jLwBYRINPqKuqU5P8diZ3nLsiySuXeKDETa21C5OktfadqvqlTEL+k1V1cSa3n31+JpfJfTCTW9ICADOYx9nyB06nuyU5Y5l1PpXkwi1vWmsfrqpnJfmtTG5Pu2eSryT5tSTv2vo+9ADAylSPOepSuHEcccQRO15pO6688sqZax9++OEdr7Qdd9xxx8y1++233IUfO2e33cZ7EOL9989+asu55547aOzXve51g+offPDBHa+0i+y+++4z1z75yU8eNPYXvrDde3ttV4//3i+Aq5e77Ht7dvXtZwGAVSbcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOuN57sxNVQ2qf9GLXjRz7Xvf+95BY++9996D6od44IEHZq696KKLBo09ZLt97nOfGzQ2sFM8zx0AEO4A0B3hDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BmPfAWAtcsjXwEA4Q4A3RHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRkc7lX16Kp6eVX9WVV9parurao7q+rTVfWyqvqebdY/oKradl4XD+0JABbZhjl8xilJ3p3km0k2Jflaku9PclKS9yQ5oapOaa21bequTfLhJT7v+jn0BAALax7hfkOS5yf5i9baw1tmVtVvJvnbJC/MJOg/tE3dNa21s+YwPgCwlcFfy7fWLmut/fnWwT6df3OS86dvjx06DgCwc+Zx5L49351OH1xi2eOr6peTPDrJbUk+21q7bhf3AwDd22XhXlUbkvz89O3HlljlJ6evrWs+meTU1trXdnKMq5ZZdMhOtgkA3dmVl8K9JcmPJLmktfbxreZvTvI7SQ5Psv/09axMTsY7NsmlVbXPLuwLALpW//Ek9jl8aNUrk7wzyReTHNNau30najYk+XSSZyY5o7X2zgHjX5XksFnrAWCNuLq1dvhKi+Z+5F5Vp2US7H+f5LidCfYkaa09mMmlc0mycd59AcCimGu4V9UZSc7L5Fr146ZnzK/Et6ZTX8sDwIzmFu5V9etJzklyTSbBfssMH3PkdPrVefUFAItmLuFeVWdmcgLdVUme3Vq7dTvrPrOq9lhi/vFJXjV9+/559AUAi2jwpXBVdWqS307yUJIrkryyqrZd7abW2oXTP781yaHTy96+MZ331CTHT/98ZmvtM0P7AoBFNY/r3A+cTndLcsYy63wqyYXTP78vyQuS/FiSE5LsnuRfkvxpkvNaa1fMoScAWFi75FK4sbkUDoBOrI1L4QCAcQl3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzvQa7geM3QAAzMEBsxRtmHMTa8V3ptOblll+yHT6xV3fSjdss9nYbrOx3VbONpvNWt5uB+Tf8mxFqrU231bWgaq6Kklaa4eP3ct6YZvNxnabje22crbZbHrdbr1+LQ8AC0u4A0BnhDsAdEa4A0BnhDsAdGYhz5YHgJ45cgeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzixUuFfVD1bVH1bVP1fV/VV1U1W9o6r2H7u3tWq6jdoyr5vH7m8sVXVyVZ1bVVdU1Xem2+P9O6g5uqouqarbq2pzVV1XVWdU1W6r1ffYVrLdquqA7ex7raouXu3+x1BVj66ql1fVn1XVV6rq3qq6s6o+XVUvq6ol/x1f9P1tpdutt/2t1+e5/wdV9aQkn0nyfUk+ksmze388yelJnltVx7TWbhuxxbXsziTvWGL+3avdyBry+iRPy2QbfCP/9kzoJVXVzyT5UJL7kvxJktuT/HSSc5Ick+SUXdnsGrKi7TZ1bZIPLzH/+jn2tZadkuTdSb6ZZFOSryX5/iQnJXlPkhOq6pS21R3J7G9JZthuU33sb621hXgl+XiSluS/bzP/f03nnz92j2vxleSmJDeN3cdaeyU5LsmTk1SSY6f70PuXWXffJLckuT/JEVvN3zOTHzhbkheP/Xdag9vtgOnyC8fue+Rtdnwmwfw928x/XCaB1ZK8cKv59rfZtltX+9tCfC1fVQcleU4mQfW/t1n8xiT3JHlpVe2zyq2xTrXWNrXWvtym/yrswMlJHpvk4tba57f6jPsyOZJNklfsgjbXnBVuN5K01i5rrf15a+3hbebfnOT86dtjt1pkf8tM260ri/K1/PHT6SeW+A99V1X9dSbhf2SSS1e7uXXgEVX1kiQ/lMkPQtcluby19tC4ba0bW/a/jy2x7PIkm5McXVWPaK3dv3ptrRuPr6pfTvLoJLcl+Wxr7bqRe1orvjudPrjVPPvbji213bboYn9blHA/eDq9YZnlX84k3J8S4b6UxyV53zbzbqyqX2ytfWqMhtaZZfe/1tqDVXVjkkOTHJTkC6vZ2Drxk9PXv6qqTyY5tbX2tVE6WgOqakOSn5++3TrI7W/bsZ3ttkUX+9tCfC2fZL/p9M5llm+Z/8hV6GW9+aMkz84k4PdJ8qNJ/iCT30/9ZVU9bbzW1g3732w2J/mdJIcn2X/6elYmJ0cdm+TSBf9V2luS/EiSS1prH99qvv1t+5bbbl3tb4sS7jtS06nfA26jtfam6e+u/qW1trm1dn1r7VcyORFxryRnjdthF+x/S2it3dJae0Nr7erW2h3T1+WZfMv2N0l+OMnLx+1yHFX1yiSvzuSqn5eutHw6Xbj9bXvbrbf9bVHCfctPqvsts3zfbdZjx7ackLJx1C7WB/vfHLXWHszkUqZkAfe/qjotyTuT/H2S41prt2+ziv1tCTux3Za0Xve3RQn3L02nT1lm+ZOn0+V+J89/dMt0um6+phrRsvvf9Pd/B2ZyYs9XV7Opde5b0+lC7X9VdUaS8zK55vq46Znf27K/bWMnt9v2rLv9bVHCfdN0+pwl7kr0vZnc1OHeJJ9b7cbWsaOm04X5B2KAy6bT5y6xbGOSvZN8ZoHPXJ7FkdPpwux/VfXrmdyE5ppMAuqWZVa1v21lBdtte9bd/rYQ4d5a+4ckn8jkJLDTtln8pkx+GruotXbPKre2plXVoVX1qCXmPzGTn4KTZLu3XCVJ8sEktyZ5cVUdsWVmVe2Z5Henb989RmNrWVU9s6r2WGL+8UleNX27EPtfVZ2ZyYlgVyV5dmvt1u2sbn+bWsl2621/q0W5l8QSt5/9QpJnZnLHrBuSHN3cfvbfqaqzkvxGJt983JjkriRPSvK8TO52dUmSF7TWHhirx7FU1YlJTpy+fVyS/5zJT/VXTOfd2lp7zTbrfzCT24FenMntQJ+fyWVLH0zyXxfhxi4r2W7Ty48OTfLJTG5VmyRPzb9dx31ma21LWHWrqk5NcmGSh5Kcm6V/V35Ta+3CrWoWfn9b6Xbrbn8b+xZ5q/lK8oRMLu36ZpIHkvxjJidYPGrs3tbiK5PLQD6QyZmld2Ry44dvJfm/mVwnWmP3OOK2OSuTs42Xe920RM0xmfxA9O1Mfg30/zI5Itht7L/PWtxuSV6W5KOZ3Fny7kxup/q1TO6V/p/G/rusoW3WknzS/jZsu/W2vy3MkTsALIqF+J07ACwS4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANCZ/w+wZP5Wcg4SYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.view(28,28).numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
